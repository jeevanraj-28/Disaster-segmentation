{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb5e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   Memory: 6.4 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "--------------------------------\n",
    "Disaster segmentation (FloodNet)\n",
    "\n",
    "Objectives:\n",
    "1. Load preprocessed data\n",
    "2. Build U-Net with pretrained encoder\n",
    "3. Train with Focal Loss (handle class imbalance)\n",
    "4. Evaluate with IoU & Dice\n",
    "5. Visualize predictions\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. Setup & Imports\n",
    "# ------------------------------------------------------------\n",
    "# Core, numerical, and plotting utilities + progress bars.\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Segmentation Models\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# Device selection\n",
    "# ------------------------------------------------------------\n",
    "# Choose GPU if available, otherwise CPU; print device info.\n",
    "# ============================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Print basic GPU specs for debugging/resource planning\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f682045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: D:\\Projects\\Image Segmentation for Disaster Resilience\\Disaster-segmentation\n",
      "Image Size: (256, 256)\n",
      "Batch Size: 8\n",
      "Epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Configuration\n",
    "# ------------------------------------------------------------\n",
    "# Project paths, model/training params, and dataset class names.\n",
    "# Short, clear comments (mix of block, inline, and docstring styles).\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "\n",
    "    # Paths\n",
    "    BASE_DIR = Path(r\"D:\\Projects\\Image Segmentation for Disaster Resilience\\Disaster-segmentation\")  # project root\n",
    "    DATA_DIR = BASE_DIR / \"data\" / \"raw\" / \"FloodNet\"  # raw FloodNet data\n",
    "\n",
    "    TRAIN_IMAGES = DATA_DIR / \"train\" / \"train-org-img\"    # train RGBs\n",
    "    TRAIN_MASKS  = DATA_DIR / \"train\" / \"train-label-img\"  # train masks\n",
    "    VAL_IMAGES   = DATA_DIR / \"val\" / \"val-org-img\"        # val RGBs\n",
    "    VAL_MASKS    = DATA_DIR / \"val\" / \"val-label-img\"      # val masks\n",
    "\n",
    "    CHECKPOINT_DIR = BASE_DIR / \"models\" / \"checkpoints\"  # where checkpoints are saved\n",
    "    LOG_DIR        = BASE_DIR / \"logs\"                    # training logs\n",
    "    RESULTS_DIR    = BASE_DIR / \"results\"                 # visualizations & metrics\n",
    "\n",
    "    # Model\n",
    "    ENCODER_NAME    = \"resnet34\"     # encoder backbone for U-Net\n",
    "    ENCODER_WEIGHTS = \"imagenet\"     # pretrained weights\n",
    "    NUM_CLASSES     = 10             # segmentation classes\n",
    "\n",
    "    # Training\n",
    "    IMG_SIZE = (256, 256) # (H, W) input size for the network\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 4\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # Early stopping\n",
    "    PATIENCE = 10\n",
    "    \n",
    "    # Class names\n",
    "    CLASS_NAMES = [\n",
    "        \"Background\", \"Building-flooded\", \"Building-non-flooded\",\n",
    "        \"Road-flooded\", \"Road-non-flooded\", \"Water\",\n",
    "        \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"\n",
    "    ]\n",
    "\n",
    "# Create directories\n",
    "Config.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "Config.LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base Directory: {Config.BASE_DIR}\")\n",
    "print(f\"Image Size: {Config.IMG_SIZE}\")\n",
    "print(f\"Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {Config.EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5251d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset module loaded\n",
      "Loaded 1445 images from D:\\Projects\\Image Segmentation for Disaster Resilience\\Disaster-segmentation\\data\\raw\\FloodNet\\train\\train-org-img\n",
      "Loaded 450 images from D:\\Projects\\Image Segmentation for Disaster Resilience\\Disaster-segmentation\\data\\raw\\FloodNet\\val\\val-org-img\n",
      "\n",
      "Dataset Summary:\n",
      "   Training: 1445 images (181 batches)\n",
      "   Validation: 450 images (57 batches)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. Dataset and DataLoaders\n",
    "# ------------------------------------------------------------\n",
    "# Load dataset module if available; otherwise define an inline\n",
    "# FloodNetDataset + simple augmentations and DataLoader setup.\n",
    "# ============================================================\n",
    "\n",
    "# Import from our modules (or define inline if not found)\n",
    "try:\n",
    "    from data.dataset import FloodNetDataset, get_train_transform, get_val_transform\n",
    "    print(\"Dataset module loaded\")\n",
    "except ImportError:\n",
    "    print(\"Using inline dataset definition\")\n",
    "\n",
    "    # Minimal inline dependencies\n",
    "    import cv2\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    class FloodNetDataset(Dataset):\n",
    "        \"\"\"\n",
    "        Simple Dataset for FloodNet.\n",
    "        - Loads RGB images and single-channel mask (class ids).\n",
    "        - Resizes to img_size and applies albumentations transforms if provided.\n",
    "        \"\"\"\n",
    "        def __init__(self, image_dir, mask_dir, transform=None, img_size=(256, 256)):\n",
    "            self.image_dir = Path(image_dir)\n",
    "            self.mask_dir = Path(mask_dir)\n",
    "            self.transform = transform\n",
    "            self.img_size = img_size\n",
    "            self.images = sorted([f for f in self.image_dir.glob(\"*\") \n",
    "                                  if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])\n",
    "            print(f\"Loaded {len(self.images)} images\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "        \n",
    "        def _get_mask_path(self, image_path):\n",
    "            \"\"\"\n",
    "            Heuristic mask lookup:\n",
    "            - Try same stem with .png\n",
    "            - Try stem + '_lab'.png\n",
    "            Returns Path or None.\n",
    "            \"\"\"\n",
    "            img_stem = image_path.stem\n",
    "            for pattern in [f\"{img_stem}.png\", f\"{img_stem}_lab.png\"]:\n",
    "                mask_path = self.mask_dir / pattern\n",
    "                if mask_path.exists():\n",
    "                    return mask_path\n",
    "            return None\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Load image (BGR -> RGB)\n",
    "            img_path = self.images[idx]\n",
    "            image = cv2.imread(str(img_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Load mask (fallback: zeros if missing)\n",
    "            mask_path = self._get_mask_path(img_path)\n",
    "            mask = (cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "                    if mask_path else np.zeros(image.shape[:2], dtype=np.uint8))\n",
    "\n",
    "            # Resize to target size (nearest for masks)\n",
    "            image = cv2.resize(image, self.img_size)\n",
    "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            # Apply augmentations or convert to tensors if none provided\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask)\n",
    "                image, mask = augmented['image'], augmented['mask']\n",
    "            else:\n",
    "                image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "                mask = torch.from_numpy(mask).long()\n",
    "\n",
    "            return image, mask\n",
    "\n",
    "    # -------------------------\n",
    "    # Augmentations (train / val)\n",
    "    # -------------------------\n",
    "    def get_train_transform(img_size=(256, 256)):\n",
    "        \"\"\"Standard augmentation pipeline for training.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=30, p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def get_val_transform(img_size=(256, 256)):\n",
    "        \"\"\"Minimal preprocessing for validation (no augmentations).\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# -------------------------\n",
    "# Instantiate datasets\n",
    "# -------------------------\n",
    "train_dataset = FloodNetDataset(\n",
    "    Config.TRAIN_IMAGES, Config.TRAIN_MASKS,\n",
    "    transform=get_train_transform(Config.IMG_SIZE),\n",
    "    img_size=Config.IMG_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = FloodNetDataset(\n",
    "    Config.VAL_IMAGES, Config.VAL_MASKS,\n",
    "    transform=get_val_transform(Config.IMG_SIZE),\n",
    "    img_size=Config.IMG_SIZE\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# DataLoaders\n",
    "# -------------------------\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True, num_workers=Config.NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False, num_workers=Config.NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "# Quick summary\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"   Training: {len(train_dataset)} images ({len(train_loader)} batches)\")\n",
    "print(f\"   Validation: {len(val_dataset)} images ({len(val_loader)} batches)\")\n",
    "\n",
    "# -------------------------\n",
    "# Sanity check: sample batch\n",
    "# -------------------------\n",
    "sample_img, sample_mask = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"   Images: {sample_img.shape}, dtype: {sample_img.dtype}\")\n",
    "print(f\"   Masks: {sample_mask.shape}, dtype: {sample_mask.dtype}\")\n",
    "print(f\"   Mask classes: {torch.unique(sample_mask).tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
